# Speech-Emotion-Recognition-Using-Deep-Learning
This project focuses on detecting human emotions from speech using deep learning techniques. We trained a neural network on the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset to classify different emotional states.
📌 Features
🔊 Audio-based emotion recognition

🧠 Deep learning model (e.g., CNN + LSTM or custom ANN)

🧪 Trained on RAVDESS dataset

📊 Emotion classes: Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised

🐍 Built with Python and PyTorch

🧠 Model Overview
Input: MFCC features extracted from .wav files

Model: Sequential deep learning model (custom ANN or CNN-LSTM)

Output: One of the 8 emotion classes

Accuracy: [Your best accuracy here — e.g., 83.4% on validation set]

🗃️ Dataset: RAVDESS
Download link: RAVDESS on Zenodo

Contains 24 professional actors vocalizing two lexically-matched statements in a neutral North American accent.

🔧 Tech Stack
Python

PyTorch / TensorFlow

Librosa (for audio feature extraction)

Scikit-learn

Matplotlib / Seaborn
