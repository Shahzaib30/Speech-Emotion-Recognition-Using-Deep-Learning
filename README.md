# Speech-Emotion-Recognition-Using-Deep-Learning
This project focuses on detecting human emotions from speech using deep learning techniques. We trained a neural network on the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) dataset to classify different emotional states.
ğŸ“Œ Features
ğŸ”Š Audio-based emotion recognition

ğŸ§  Deep learning model (e.g., CNN + LSTM or custom ANN)

ğŸ§ª Trained on RAVDESS dataset

ğŸ“Š Emotion classes: Neutral, Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised

ğŸ Built with Python and PyTorch

ğŸ§  Model Overview
Input: MFCC features extracted from .wav files

Model: Sequential deep learning model (custom ANN or CNN-LSTM)

Output: One of the 8 emotion classes

Accuracy: [Your best accuracy here â€” e.g., 83.4% on validation set]

ğŸ—ƒï¸ Dataset: RAVDESS
Download link: RAVDESS on Zenodo

Contains 24 professional actors vocalizing two lexically-matched statements in a neutral North American accent.

ğŸ”§ Tech Stack
Python

PyTorch / TensorFlow

Librosa (for audio feature extraction)

Scikit-learn

Matplotlib / Seaborn
